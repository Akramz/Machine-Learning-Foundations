{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a collection of points $\\{x_1,x_2,..,x_m\\} \\in \\Bbb{R}^{m}, \\  \\forall i \\in \\{1,..,m\\} \\ x_{i} \\in \\Bbb{R}^{n \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to apply lossy compression to these points, meaning we will reduce their dimensionality resulting in less required memory but also less precision. when decoding the compressed points back to their original form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding Function:\n",
    "$$f: \\Bbb{R}^{n \\times 1} \\to \\Bbb{R}^{l \\times 1} \\\\ x \\to c$$\n",
    "With $l<n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the decoding function, we choose:\n",
    "\n",
    "$$g: \\Bbb{R}^{l \\times 1} \\to \\Bbb{R}^{n \\times 1} \\\\ c \\to Dc \\approx x$$\n",
    "\n",
    "with $D \\in \\Bbb{R}^{n \\times l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Constrain:\n",
    "* $\\forall i \\   D_{:,i}$ to be orthogonal.\n",
    "* $\\forall i \\   D_{:,i} $ to have unit Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to find the optimal $f$ for all $x$.\n",
    "\n",
    "One way to do this is to minimize the distance between the input point $x$ and its reconstruction $g(c^{*})$, We can measure this distance using the Norm, in PCA, we use $L^{2}$:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x - g(c) \\rVert_{2} \\ / \\ c=f(x)$$\n",
    "\n",
    "So we are looking for $f$ by minimizing the Compressions Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L^2$ is non-negative and $x \\to x^2$ is monotonically increasing for positive arguments, we can do this:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x-g(c) \\rVert^{2}_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize:\n",
    "\n",
    "$$G(c) = (x-g(c))^{T}(x-g(c))\\\\\n",
    "G(c) = x^{T}x - x^{T}g(c) - g(c)^{T}x + g(c)^{T}g(c)\\\\\n",
    "G(c) = x^{T}x - 2x^{T}g(c) + g(c)^{T}g(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the first term of $G(c)$ because it doesn't depend on $c$:\n",
    "$$G(c) = - 2x^{T}g(c) + g(c)^{T}g(c)\\\\\n",
    "G(c)=(Dc)^{T}(Dc) - 2x^{T}(Dc)\\\\\n",
    "G(c)=c^{T}D^{T}Dc - 2x^{T}Dc\\\\\n",
    "G(c)=c^{T}c - 2x^{T}Dc$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimize $G$ using Vector Calculus:\n",
    "\n",
    "$$\\nabla_{c}(c^{T}c - 2x^{T}Dc)=0\\\\\n",
    "2c - 2D^{T}x=0\\\\\n",
    "c = D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "$$f(x)=D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a further Matrix multiplication, we can also define the PCA reconstruction operation:\n",
    "\n",
    "$$r(x)=g(f(x))=DD^{T}x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
