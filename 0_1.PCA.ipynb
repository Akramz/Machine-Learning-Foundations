{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a collection of points $\\{x_1,x_2,..,x_m\\} \\in \\Bbb{R}^{m}, \\  \\forall i \\in \\{1,..,m\\} \\ x_{i} \\in \\Bbb{R}^{n \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to apply lossy compression to these points, meaning we will reduce their dimensionality resulting in less required memory but also less precision. when decoding the compressed points back to their original form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding Function:\n",
    "$$f: \\Bbb{R}^{n \\times 1} \\to \\Bbb{R}^{l \\times 1} \\\\ x \\to c$$\n",
    "With $l<n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the decoding function, we choose:\n",
    "\n",
    "$$g: \\Bbb{R}^{l \\times 1} \\to \\Bbb{R}^{n \\times 1} \\\\ c \\to Dc \\approx x$$\n",
    "\n",
    "with $D \\in \\Bbb{R}^{n \\times l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Constrain:\n",
    "* $\\forall i \\   D_{:,i}$ to be orthogonal.\n",
    "* $\\forall i \\   D_{:,i} $ to have unit Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Finding $ \\ f$\n",
    "\n",
    "The first thing to do is to find the optimal $f$ for all $x$.\n",
    "\n",
    "One way to do this is to minimize the distance between the input point $x$ and its reconstruction $g(c^{*})$, We can measure this distance using the Norm, in PCA, we use $L^{2}$:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x - g(c) \\rVert_{2} \\ / \\ c=f(x)$$\n",
    "\n",
    "So we are looking for $f$ by minimizing the Compressions Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L^2$ is non-negative and $x \\to x^2$ is monotonically increasing for positive arguments, we can do this:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x-g(c) \\rVert^{2}_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize:\n",
    "\n",
    "$$G(c) = (x-g(c))^{T}(x-g(c))\\\\\n",
    "G(c) = x^{T}x - x^{T}g(c) - g(c)^{T}x + g(c)^{T}g(c)\\\\\n",
    "G(c) = x^{T}x - 2x^{T}g(c) + g(c)^{T}g(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the first term of $G(c)$ because it doesn't depend on $c$:\n",
    "$$G(c) = - 2x^{T}g(c) + g(c)^{T}g(c)\\\\\n",
    "G(c)=(Dc)^{T}(Dc) - 2x^{T}(Dc)\\\\\n",
    "G(c)=c^{T}D^{T}Dc - 2x^{T}Dc\\\\\n",
    "G(c)=c^{T}c - 2x^{T}Dc$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimize $G$ using Vector Calculus:\n",
    "\n",
    "$$\\nabla_{c}(c^{T}c - 2x^{T}Dc)=0\\\\\n",
    "2c - 2D^{T}x=0\\\\\n",
    "c = D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "$$f(x)=D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a further Matrix multiplication, we can also define the PCA reconstruction operation:\n",
    "\n",
    "$$r(x)=g(f(x))=DD^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Finding $D$\n",
    "\n",
    "Next step is to choose the encoding matrix $D$:\n",
    "\n",
    "Since we'll use the same matrix $D$ to decode all points, we can no longer consider the points in isolation.\n",
    "\n",
    "We will minimize the Frobenius Norm of the matrix of errors computer over all dimensions and all points (Total Loss):\n",
    "\n",
    "$$D^{*}=arg_{D}min \\sqrt{\\sum_{i,j}(x^{(i)}_j -r(x^{(i)})_j)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the algorithm for finding $D^*$, we start by considering $l=1$ meaning $D \\in \\Bbb{R}^{n \\times 1}$ is a vector, $d$:\n",
    "\n",
    "$$d^{*}=arg_dmin \\sum_{i} \\lVert x^{(i)} - dd^{T}x^{(i)} \\rVert^{2}_{2}\\\\\n",
    "d^{*}=arg_dmin \\sum_{i} \\lVert x^{(i)} - (x^{(i)})^Tdd \\rVert^{2}_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\Bbb{R}^{m \\times n} \\ , \\ X_{i,:} = (x^{(i)})^T$:\n",
    "\n",
    "We can Now rewrite the Problem as:\n",
    "\n",
    "$$d^{*}=arg_dmin \\ \\lVert X - Xdd^T \\rVert^{2}_{F} \\\\\n",
    "d^{*}=arg_dmin \\ \\{-2Tr(X^{T}Xdd^{T}) + Tr(dd^{T}X^{T}Xdd^{T})\\} \\\\\n",
    "d^{*}=arg_dmax \\ Tr(d^{T}X^{T}Xd)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Optimization problem is may be solved using eigen decomposition. \n",
    "\n",
    "Specifically, the Optimal $d$ is given by the eigen vector of $X^{T}X$ Corresponding to the Largest Eigenvalue.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Sklearn Example\n",
    "\n",
    "We will try to find $d$ in the simplest scenario, we'll calculate it using numpy, Let's do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\{(-1,-1);(-2,-1);(-3,-2);(1,1);(2,1);(3,2)\\} \\in \\Bbb{R}^{2} \\ / \\ x_1, x_2 \\in \\Bbb{R}^{2,1}$, $l=1$, So:\n",
    "\n",
    "$D=d=\\begin{bmatrix} d_1 \\\\ d_2 \\end{bmatrix}  \\in \\Bbb{R}^{2 \\times 1}$, \n",
    "and $X=\\begin{bmatrix} -1 & -1 \\\\ -2 & -1 \\\\ -3 & -2 \\\\ 1 & 1 \\\\ 2 & 1 \\\\ 3 & 2 \\end{bmatrix}$, and finally:\n",
    "\n",
    "$$f: \\Bbb{R}^{2 \\times 1} \\to \\Bbb{R}^{1 \\times 1} \\\\ x \\to d^{T}x$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$g: \\Bbb{R}^{1 \\times 1} \\to \\Bbb{R}^{2 \\times 1} \\\\ c \\to dc$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $d$ to minimize the the distance between the reconstructions $dd^{T}x$ and the original points $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-2, -1],\n",
       "       [-3, -2],\n",
       "       [ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [ 3,  2]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]);X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We center the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = X_ - X_.mean(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate $X^{T}X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.matmul(X__.T, X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_values, eig_vectors = np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigen vector with the largest eigen value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reconstruct the the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.]\n",
      "[-1. -1.]\n",
      "[-1.5 -1.5]\n",
      "[-1.5 -1.5]\n",
      "[-2.5 -2.5]\n",
      "[-2.5 -2.5]\n",
      "[1. 1.]\n",
      "[1. 1.]\n",
      "[1.5 1.5]\n",
      "[1.5 1.5]\n",
      "[2.5 2.5]\n",
      "[2.5 2.5]\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(eig_vectors[0].dot(eig_vectors[0].T.dot(x)))\n",
    "    print(eig_vectors[0].dot(eig_vectors[0].T.dot(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "to automate the process of dimensionality reduction, we can simply use Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-2, -1],\n",
       "       [-3, -2],\n",
       "       [ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [ 3,  2]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]);X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedakramzaytar/.envs/kaggle/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we scale X as we did using Numpy.\n",
    "pca.fit(StandardScaler().fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.]\n",
      "[-1.5 -1.5]\n",
      "[-2.5 -2.5]\n",
      "[1. 1.]\n",
      "[1.5 1.5]\n",
      "[2.5 2.5]\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    print(pca.inverse_transform(pca.transform([x]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
