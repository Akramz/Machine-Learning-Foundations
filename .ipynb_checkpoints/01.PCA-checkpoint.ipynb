{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a collection of points $\\{x_1,x_2,..,x_m\\} \\in \\Bbb{R}^{m}, \\  \\forall i \\in \\{1,..,m\\} \\ x_{i} \\in \\Bbb{R}^{n \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to apply lossy compression to these points, meaning we will reduce their dimensionality resulting in less required memory but also less precision. when decoding the compressed points back to their original form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding Function:\n",
    "$$f: \\Bbb{R}^{n \\times 1} \\to \\Bbb{R}^{l \\times 1} \\\\ x \\to c$$\n",
    "With $l<n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the decoding function, we choose:\n",
    "\n",
    "$$g: \\Bbb{R}^{l \\times 1} \\to \\Bbb{R}^{n \\times 1} \\\\ c \\to Dc \\approx x$$\n",
    "\n",
    "with $D \\in \\Bbb{R}^{n \\times l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Constrain:\n",
    "* $\\forall i \\   D_{:,i}$ to be orthogonal.\n",
    "* $\\forall i \\   D_{:,i} $ to have unit Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Finding $ \\ f$\n",
    "\n",
    "The first thing to do is to find the optimal $f$ for all $x$.\n",
    "\n",
    "One way to do this is to minimize the distance between the input point $x$ and its reconstruction $g(c^{*})$, We can measure this distance using the Norm, in PCA, we use $L^{2}$:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x - g(c) \\rVert_{2} \\ / \\ c=f(x)$$\n",
    "\n",
    "So we are looking for $f$ by minimizing the Compressions Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L^2$ is non-negative and $x \\to x^2$ is monotonically increasing for positive arguments, we can do this:\n",
    "\n",
    "$$c^{*}=arg_{c}min \\lVert x-g(c) \\rVert^{2}_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize:\n",
    "\n",
    "$$G(c) = (x-g(c))^{T}(x-g(c))\\\\\n",
    "G(c) = x^{T}x - x^{T}g(c) - g(c)^{T}x + g(c)^{T}g(c)\\\\\n",
    "G(c) = x^{T}x - 2x^{T}g(c) + g(c)^{T}g(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the first term of $G(c)$ because it doesn't depend on $c$:\n",
    "$$G(c) = - 2x^{T}g(c) + g(c)^{T}g(c)\\\\\n",
    "G(c)=(Dc)^{T}(Dc) - 2x^{T}(Dc)\\\\\n",
    "G(c)=c^{T}D^{T}Dc - 2x^{T}Dc\\\\\n",
    "G(c)=c^{T}c - 2x^{T}Dc$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimize $G$ using Vector Calculus:\n",
    "\n",
    "$$\\nabla_{c}(c^{T}c - 2x^{T}Dc)=0\\\\\n",
    "2c - 2D^{T}x=0\\\\\n",
    "c = D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "$$f(x)=D^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a further Matrix multiplication, we can also define the PCA reconstruction operation:\n",
    "\n",
    "$$r(x)=g(f(x))=DD^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Finding $D$\n",
    "\n",
    "Next step is to choose the encoding matrix $D$:\n",
    "\n",
    "Since we'll use the same matrix $D$ to decode all points, we can no longer consider the points in isolation.\n",
    "\n",
    "We will minimize the Frobenius Norm of the matrix of errors computer over all dimensions and all points (Total Loss):\n",
    "\n",
    "$$D^{*}=arg_{D}min \\sqrt{\\sum_{i,j}(x^{(i)}_j -r(x^{(i)})_j)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the algorithm for finding $D^*$, we start by considering $l=1$ meaning $D \\in \\Bbb{R}^{n \\times 1}$ is a vector, $d$:\n",
    "\n",
    "$$d^{*}=arg_dmin \\sum_{i} \\lVert x^{(i)} - dd^{T}x^{(i)} \\rVert^{2}_{2}\\\\\n",
    "d^{*}=arg_dmin \\sum_{i} \\lVert x^{(i)} - (x^{(i)})^Tdd \\rVert^{2}_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\Bbb{R}^{m \\times n} \\ , \\ X_{i,:} = (x^{(i)})^T$:\n",
    "\n",
    "We can Now rewrite the Problem as:\n",
    "\n",
    "$$d^{*}=arg_dmin \\ \\lVert X - Xdd^T \\rVert^{2}_{F} \\\\\n",
    "d^{*}=arg_dmin \\ \\{-2Tr(X^{T}Xdd^{T}) + Tr(dd^{T}X^{T}Xdd^{T})\\} \\\\\n",
    "d^{*}=arg_dmax \\ Tr(d^{T}X^{T}Xd)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Optimization problem is may be solved using eigen decomposition. \n",
    "\n",
    "Specifically, the Optional $d$ is given by the eigen vector of $X^{T}X$ Corresponding to the Largest Eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
