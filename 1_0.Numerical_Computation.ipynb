{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algorithms usually require a high amount of numerical computation.\n",
    "\n",
    "This typically refers to algorithms that solve mathematical problems by methods that update estimates of the solution via an iterative process, rather than analytically deriving a formula to provide a symbolic expression for the correct solution.\n",
    "\n",
    "Common operations include function optimization and solving systems of linear equations.\n",
    "\n",
    "Even just evaluating a mathematical function on a digital computer can be difficult when the function involves real numbers, which cannot be represented precisely using a finite amount of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overflow & Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite numbers of bit patterns.\n",
    "\n",
    "This means that for almost all real numbers, we incur some approximation error when we represent the number in the computer. In many cases, this is just rounding error.\n",
    "\n",
    "For example, we know that the decimal representation of $\\pi$ in never ending, so if we wanted to use $pi$ in a computer, we have to `round` it to a certain decimal precision, Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpmath import mp\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.pi; pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a more precise value of $\\pi$ and calculate the error between numpy's `pi` and our $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.dps = 33  # precision.\n",
    "our_pi = mp.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926535897932384626433832795\n"
     ]
    }
   ],
   "source": [
    "print(our_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _mpf.ae of <pi: 3.14159~>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_pi.ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2246467991473532e-16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = our_pi - pi; float(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounding Error is problematic, especially when it compounds across many operations, and can cause algorithms that work in theory to fail in practice if they are not designed to minimize the accumulation of rounding error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One form of rounding error that is particularly devastating is **Underflow**. Underflow occurs when numbers near zero are rounded to zero.<br/>\n",
    "Many functions behave qualitatively differently when their argument is zero rather than a small positive number.\n",
    "\n",
    "For example, we usually want to avoid division by zero or taking the logarithm of zero.\n",
    "\n",
    "Another highly damaging form of numerical error is **Overflow**. Overflow occurs when numbers with large magnitude are approximated as $\\infty$ or $-\\infty$.<br/>\n",
    "Further arithmetic will usually change these infinite values into not-a-number values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a function that must be stabilized against overflow and underflow is the **Softmax** function. The softmax function is often used to predict the probabilities associated with N classes of possible predictions (Example: Predict {Rainy, Sunny, Cloudy}), these probabilities are associated with a multinoulli distribution.\n",
    "\n",
    "The softmax function considers a pre-defined vector $x=\\{x_1, x_2, ..., x_n\\}$ and is defined to be from $\\Bbb{R} \\to [0,1]$ and:\n",
    "\n",
    "$$softmax(x)_i = {{e^{x_i}} \\over {\\sum^{n}_{j=1}e^{x_j}}}$$\n",
    "\n",
    "Let's implement it using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vector.\n",
    "x = np.array([2, 45, 64, 22, 7, 9, 13], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, x_i):\n",
    "    '''\n",
    "    Calculates the Softmax of an element considering the whole vector.\n",
    "    Input: Vector of all elements x, and element x_i that we want to calculate its softmax.\n",
    "    Output: Softmax(x_i).\n",
    "    '''\n",
    "    # to debug the nan.\n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    upper = np.exp(x_i)\n",
    "    bottom = sum([np.exp(x_j) for x_j in x])\n",
    "    return upper / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.99999943972034"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[2])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the softmax is a probability measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([softmax(x, x_i) for x_i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what happens when all the $x_i$ are equal to some constant $c$.\n",
    "\n",
    "Analytically, we can see that all of probabilities should be equal to $1 \\over n$.\n",
    "\n",
    "But numerically, this may not occur when $c$ has large magnitude either in the left or the right direction.\n",
    "\n",
    "Let's test this using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [999999999999*99999999999 for _ in range(10)]\n",
    "x = np.array(l, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax output for any input should be $10%$, let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-20-5e54f51395e9>\u001b[0m(10)\u001b[0;36msoftmax\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m    \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  x_i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e+23\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.exp(x_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  sum([np.exp(x_j) for x_j in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a `nan` because the exponential of a big number is `inf` and so `inf` over `inf` is undefined. This is what's called the overflow problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second example, Let's take numbers that are very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [-999999999999*99999999999 for _ in range(10)]\n",
    "x = np.array(l, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-20-5e54f51395e9>\u001b[0m(10)\u001b[0;36msoftmax\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m    \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.exp(x_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  sum([np.exp(x_j) for x_j in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is `nan` because the exponential of a very big but negative number rounds to 0, and 0 over 0 is undefined. This is what's called the underflow problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these difficulties can be resolved by instead evaluating softmax(z) where $z=x-max_i(x_i)$. Let's write this new softmax function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, x_i):\n",
    "    '''\n",
    "    Calculates the Softmax of an element considering the whole vector.\n",
    "    Input: Vector of all elements x, and element x_i that we want to calculate its softmax.\n",
    "    Output: Softmax(x_i).\n",
    "    '''\n",
    "    # to debug the nan.\n",
    "    #ipdb.set_trace()\n",
    "    \n",
    "    n_x_i = x_i - max(x)\n",
    "    n_x   = [x_j - max(x) for x_j in x]\n",
    "    \n",
    "    upper = np.exp(n_x_i)\n",
    "    bottom = sum([np.exp(x_j) for x_j in n_x])\n",
    "    return upper / bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it for big negative numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's test it for big positive numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [999999999999*99999999999 for _ in range(10)]\n",
    "x = np.array(l, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's working because we scale the elements in the tensor, here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.63429843e+100, 9.23631147e+099, 6.44757050e+100, 6.47735934e+100,\n",
       "       6.36622260e+100, 7.43810763e+100, 3.64323466e+100, 2.36613059e+100,\n",
       "       1.03367117e+100, 7.20193653e+100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big positive numbers.\n",
    "x = np.random.rand(10,)*100e99;x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-inf, -inf, -inf, -inf, -inf,   0., -inf, -inf, -inf, -inf],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale.\n",
    "np.array([x_i - max(x) for x_i in x], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x, x[1]), softmax(x, x[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple algebra shows that the value of the softmax function is not changed analytically by adding or substracting a scalar from the elements of the vector.\n",
    "\n",
    "Substracting $max_i x_i$ results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still one small problem, as we saw in the previous, if there is a big margin between one element and the max, So underflow in the numerator can still cause the expression to evaluate to zero. This means that if we implement logsoftmax(x) by first running the softmax subroutine then passing the result to the log function, we could erroneously obtain $-\\infty$.\n",
    "\n",
    "Instead, we must implement a separate function that calculate logsoftmax in a numerical stable way. The logsoftmax function can be stabilized using the same trick as we used to stabilize the softmax function.\n",
    "\n",
    "Theano is an example of a software package that automatically detects and stabilizes many common numerically unstable expressions that arise in the context of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poor Conditionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $f(x)=\\bf{A}^{-1}x$. When $\\bf{A} \\in \\Bbb{R}^{n \\times n}$ has an eigenvalue decomposition, its **condition number** is:\n",
    "\n",
    "$$max_{i,j} |{\\lambda_{i} \\over \\lambda_{j}}|$$\n",
    "\n",
    "This is the ratio of the magnitude of the largest and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive to error in the input.\n",
    "\n",
    "This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse. In practice, the error will be compounded further by numerical errors in the inversion process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function $f(x)$ by altering $x$. We usually phrase most optimization problems in terms of minimizing $f(x)$.\n",
    "\n",
    "The function that we want to minimize or maximize is usually called the **objective function**, or **criterion**. When we are minimizing it, we may also call it the **cost function**, **loss function**, or **error function**.\n",
    "\n",
    "We denote the value that minimizes a function $f(x)$ using $*$, we might say: $x^{*} = argmin f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a function $y=f(x)$, where both $x$ and $y$ are real numbers.\n",
    "\n",
    "The derivative of this function is denoted as $f'(x)$ or as ${df} \\over {dx}$, the derivative $f'(x)$ gives the slope of $f(x)$ at the point x. In other words, it specifies how to scale a small change in the input to obtain the corresponding change in the output: $f(x+\\epsilon) = f(x) + \\epsilon f'(x)$.\n",
    "\n",
    "The derivative is therefore useful for minimizing a function because it tells us how to change $x$ in order to make a small improvement in $y$.\n",
    "\n",
    "For example, we know that $f(x-\\epsilon \\cdot sign(f'(x)))$ is less than $f(x)$ for a small enough $\\epsilon$. we can thus reduce $f(x)$ by moving $x$ in small steps in the opposite direction of the derivative. This technique is called **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an example in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''\n",
    "    Function: f(x)=(x^2)/2\n",
    "    Input: x.\n",
    "    Output: (x^2)/2.\n",
    "    '''\n",
    "    return np.around((x*x)/2, decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    '''\n",
    "    The Gradient of f(x)=(x^2)/2, it's grad_f(x)=x.\n",
    "    Input: x.\n",
    "    Output: x.\n",
    "    '''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate some points.\n",
    "x = np.arange(start=-10, stop=10, step=0.1, dtype='float32')\n",
    "y = [f(x_i) for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the gradients.\n",
    "grad_y = [grad_f(x_i) for x_i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct three slope functions, in:\n",
    "* $x=-1.5$\n",
    "* $x=0$\n",
    "* $x=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three points of interest.\n",
    "x_1 = -1.5\n",
    "x_2 = 0\n",
    "x_3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first thing is to get the slopes in the corresponding points.\n",
    "a_1 = grad_f(x_1)\n",
    "a_2 = grad_f(x_2)\n",
    "a_3 = grad_f(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we draw the corresponding three functions around the points.\n",
    "x_1_range = [np.around(x_1_i, decimals=1) for x_1_i in np.arange(start=x_1-0.3, stop=x_1+0.3+0.1, step=0.1, dtype='float16')]\n",
    "x_2_range = [np.around(x_2_i, decimals=1) for x_2_i in np.arange(start=x_2-0.3, stop=x_2+0.3+0.1, step=0.1, dtype='float16')]\n",
    "x_3_range = [np.around(x_3_i, decimals=1) for x_3_i in np.arange(start=x_3-0.3, stop=x_3+0.3+0.1, step=0.1, dtype='float16')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the slope function:\n",
    "\n",
    "$$y = f'(x_0)x - f(x_0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the gradient y_s corresponding to the three ranges.\n",
    "g_1 = [(a_1*x_i) - f(x_1) for x_i in x_1_range]\n",
    "g_2 = [(a_2*x_i) - f(x_2) for x_i in x_2_range]\n",
    "g_3 = [(a_3*x_i) - f(x_3) for x_i in x_3_range]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the function, It's derivative function, and some slope examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot(x, y, '--', color='blue')\n",
    "plt.plot(x, grad_y, color='green')\n",
    "\n",
    "plt.plot(x_1_range, g_1, color='black')\n",
    "plt.plot(x_2_range, g_2, color='black')\n",
    "plt.plot(x_3_range, g_3, color='black')\n",
    "\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $f'(x)=0$, the derivative provides to information about which direction to move. Points where $f'(x)=0$ are known as **critical points**, or **stationary points**. A **Local Minimum** is a point where $f(x)$ is lower than all neighbouring points, so it is no longer possible to decrease $f(x)$ by making infitesimal steps.\n",
    "\n",
    "Some critial points are neither maxima or minima, These are known as **saddle points**.\n",
    "\n",
    "<img src=\"imgs/critical_points_types.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
